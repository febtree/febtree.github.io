---
layout: post
title: 大规模数据的相似度计算：LSH算法
key: 10002
tags: 推荐系统 协同过滤 Min-Hashing LSH 局部敏感哈希
category: algorithm
date: 2018-08-11 19:42:00 +08:00
modify_date: 2018-08-11 19:42:00 +08:00
---

### 前言

最近在工作中需要去优化离职同事留下的用户协同过滤算法，本来想协同过滤嘛，不就是一顿算相似度，然后取top-k相似的用户去做推荐就完了。结果看代码的过程中，对计算相似度的部分却是一头雾水，主要是对其中使用的LSH算法不甚了解。经过了一番调研之后，才算是理解了这个算法的精妙，也感觉自己之前的粗糙想法实在是naive。

传统的协同过滤算法，不管是基于用户还是基于物品的，其中最关键的一个问题便是：计算两个用户（或物品）之间的相似度。相似度的计算有多种方式：欧氏距离、余弦相似度或者Jaccard相似度，不管以何种计算方式，在数据维度较小时，都可以用naive的方式直接遍历每一个pair去计算。但当数据维度增大到一定程度时，计算复杂度就开始飙升了，主要体现在两个方面（以计算用户相似度为例）：

1. 两个用户之间相似度的计算随着物品维度的增加而增加
2. 计算每一个用户和其他所有用户之间的相似度的复杂度随着用户规模的增长，呈平方增长

对于工业界的数据，用户和物品的维度都在千万甚至更高的情况下，直接计算两两之间的相似度，即便使用大规模计算集群有可能实现，所需要的计算成本也是极高的。这时便需要使用近似算法，牺牲一些精度来大大提高计算效率。Min Hashing和Locality Sensitive Hashing（LSH，局部敏感哈希）便是用来分别提高这两个方面的计算效率的。

### Min Hashing

首先我们定义一下变量的记号：假设有两个用户，用向量A和B来表示，其长度为n（也就是item的维度）。A和B向量中的非零值个数分别为$a$和$b$，A、B向量中共同的非零值个数为$c$，则Jaccard相似度可定义为：

$$ Jaccard(A,B) = \frac{c}{a+b} $$

当a，b的值较大的话，计算Jaccard相似度的复杂度也是线性增长的，如何去减小这个计算复杂度就是MinHash想要去解决的问题。简单来说，MinHash所做的事情就是：**将向量A、B映射到一个低维空间，并且近似保持A、B之间的相似度。**

如何得到这样的映射呢？我们现将用户A、B用物品向量的形式表现如下

<center>
<img src="https://raw.githubusercontent.com/febtree/febtree.github.io/master/_posts/img/MinHashSample.jpg" alt="MinHashSample">
</center>

其中$i_1$到$i_n$表示n个物品


### Locality Sensitive Hashing

... ...